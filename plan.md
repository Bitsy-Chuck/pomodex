# Agent Sandbox Platform — Build Strategy

> **Phase 1 stack**: Docker on GCP VM  
> **Phase 2**: Firecracker microVMs (same everything else)  
> **Users**: Multi-tenant  
> **LLM**: Called directly from mobile app (keys on device)  
> **Agent**: Claude Code CLI running inside each sandbox  
> **GCS auth**: One GCP service account per project  
> **Snapshots**: Docker image (system state) → Artifact Registry | /home/agent files (disaster recovery) → GCS  
> **SSH**: One keypair per project, generated by Project Service  
> **Scale at launch**: < 10 concurrent sandboxes  
> **Networking**: IP-based locally (no domain yet)

---

## 1. System Overview

```
Mobile App / Web
        │  JWT auth  │  REST API
        ▼
  Project Service (FastAPI)         ← single backend, runs on GCP VM
        │
   ┌────┼──────────────────────┐
   │    │                      │
   ▼    ▼                      ▼
Docker  PostgreSQL           GCP IAM
Engine  (users, projects,    (create/delete service
         metadata)            accounts per project)
   │
   ├── sandbox-{project_id}          ← one container per project
   │       ├── /home/agent           ← Docker named volume (hot storage)
   │       ├── /mnt/gcs              ← gcsfuse → gs://bucket/projects/{id}/
   │       ├── /mnt/shared           ← gcsfuse → gs://bucket/shared/ (read-only)
   │       ├── sshd :22
   │       ├── ttyd :7681            ← terminal over WebSocket
   │       ├── Claude Code CLI       ← agent, runs inside the terminal session
   │       └── backup daemon         ← rclone sync every 5 min
   │
   └── postgres

Terminal Proxy container (network_mode: host):
   └── terminal-proxy                 ← port 9000, WebSocket proxy, audit log, Squid ACL mgmt

Host services (systemd):
   └── squid                          ← egress proxy, port 3128 (explicit proxy, not transparent)

GCP Storage:
   ├── Artifact Registry  ← Docker image (system state: packages, configs, tools)
   │                         fast restore — full container state baked in
   └── GCS bucket         ← /home/agent files only (disaster recovery, file-level access)
                             used only if volume is lost/corrupted
```

**Data flow for an agent interaction:**
1. User opens terminal view in app → connects to Terminal Proxy (port 9000)
2. User types `claude` → Claude Code CLI starts inside the tmux session
3. User chats with Claude Code in the terminal — it reads/writes files and runs commands natively
4. Everything is visible in real time in the terminal view
5. All file changes are in `/home/agent` → backup daemon syncs to GCS every 5 min

---

## 2. Data Model

```sql
-- Users
CREATE TABLE users (
    id            UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email         TEXT UNIQUE NOT NULL,
    password_hash TEXT NOT NULL,
    created_at    TIMESTAMPTZ DEFAULT now()
);

-- Refresh tokens
CREATE TABLE refresh_tokens (
    id         UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id    UUID REFERENCES users(id) ON DELETE CASCADE,
    token_hash TEXT UNIQUE NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Projects
CREATE TABLE projects (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id         UUID REFERENCES users(id) ON DELETE CASCADE,
    name            TEXT NOT NULL,
    status          TEXT NOT NULL DEFAULT 'creating',
    -- status: creating | running | stopped | snapshotting | restoring | error

    -- Docker
    container_id    TEXT,
    container_name  TEXT,              -- sandbox-{id}
    volume_name     TEXT,              -- vol-{id}
    ssh_host_port   INT,               -- random port mapped to container :22
    -- ttyd port is NOT mapped to the host — proxy connects via container's Docker network IP

    -- SSH
    ssh_public_key  TEXT NOT NULL,     -- injected into container at start
    ssh_private_key TEXT NOT NULL,     -- stored in DB; returned to client on creation and retrievable for SSH

    -- GCP / GCS
    gcp_service_account_email TEXT,    -- sa-{short_id}@project.iam.gserviceaccount.com
    gcp_service_account_key   TEXT,    -- JSON key, injected into container at start
    gcs_prefix      TEXT NOT NULL,     -- projects/{id}/

    -- Snapshots (Artifact Registry)
    snapshot_image      TEXT,          -- {region}-docker.pkg.dev/{gcp_project}/sandboxes/{project_id}:latest
    last_snapshot_at    TIMESTAMPTZ,
    snapshot_size_bytes BIGINT,

    -- Metadata
    created_at      TIMESTAMPTZ DEFAULT now(),
    last_active_at  TIMESTAMPTZ DEFAULT now(),
    last_backup_at  TIMESTAMPTZ,
    last_connection_at TIMESTAMPTZ     -- updated on each ttyd WebSocket connect, drives inactivity check
);
```

**Multi-tenancy isolation rule**: every query in the Project Service filters by `user_id` from the JWT. A user can never touch another user's project.

---

## 3. Authentication

### 3.1 User auth (Project Service)

Simple JWT flow:

```
POST /auth/register  { email, password }  →  { user_id }
POST /auth/login     { email, password }  →  { access_token, refresh_token }
POST /auth/refresh   { refresh_token }    →  { access_token }
```

- Passwords: bcrypt
- Access token: short-lived JWT (15 min), signed with `JWT_SECRET` env var
- Refresh token: long-lived (30 days), stored in `refresh_tokens` table
- All other endpoints: require `Authorization: Bearer <token>` header
- Middleware extracts `user_id` from token and injects into request context

### 3.2 GCS auth (inside containers)

Each project gets its own GCP service account scoped only to its GCS prefix.

**Service account name**: `sa-{project_id_short}@{gcp_project}.iam.gserviceaccount.com`

**IAM binding** (created by Project Service at project creation time):
```
gs://your-bucket/projects/{project_id}/*  →  roles/storage.objectAdmin
gs://your-bucket/shared/*                 →  roles/storage.objectViewer
```

This uses GCS conditional IAM — the service account literally cannot read or write any other project's data even if the container were compromised.

**Key injection**: Project Service generates a JSON key and passes it to the container as env var `GCS_SA_KEY`. The entrypoint writes it to `/tmp/gcs-key.json` and sets `GOOGLE_APPLICATION_CREDENTIALS`. Both gcsfuse and rclone pick this up automatically.

**Cleanup**: when a project is deleted, Project Service calls the IAM API to delete the service account entirely.

---

## 4. Project Service API

```
POST   /auth/register
POST   /auth/login
POST   /auth/refresh

GET    /projects                    list user's projects
POST   /projects                    create project → spawn container
GET    /projects/{id}               status + connection info
POST   /projects/{id}/start         start stopped container
POST   /projects/{id}/stop          final backup + stop container
DELETE /projects/{id}               stop + final backup + destroy container + delete SA
POST   /projects/{id}/restore       restore volume from GCS + start container
POST   /projects/{id}/snapshot      trigger manual point-in-time snapshot to GCS
GET    /projects/{id}/backup-status last backup time, GCS usage
```

**`POST /projects` response:**
```json
{
  "project_id": "abc123",
  "status": "running",
  "ssh": {
    "host": "34.x.x.x",
    "port": 32456,
    "user": "agent",
    "private_key": "-----BEGIN OPENSSH PRIVATE KEY-----\n..."
  },
  "terminal_url": "ws://34.x.x.x:9000/terminal/abc123?token=..."
}
```

> **Note**: `private_key` is stored in the DB and returned in the `POST /projects` response. The client stores it in device Keychain / localStorage for SSH access. One-time display to the user (show-once-then-hide UX) is a future improvement.

---

## 5. Project Lifecycle

### 5.1 Create

```python
async def create_project(user_id, name):
    project_id = uuid4()

    # 1. Create GCP service account
    sa_email = create_gcp_service_account(project_id)
    sa_key   = create_gcp_sa_key(sa_email)
    grant_gcs_iam(sa_email, bucket, f"projects/{project_id}/")

    # 2. Generate SSH keypair
    private_key, public_key = generate_ed25519_keypair()

    # 3. Create Docker volume
    docker.volumes.create(f"vol-{project_id}")

    # 4. Find free port for SSH
    ssh_port = find_free_port()

    # 5. Insert into DB (status = 'creating')
    insert_project(project_id, user_id, name, ssh_port,
                   public_key, private_key, sa_email, sa_key)

    # 6. Start container — use snapshot image if one exists, else base image
    project    = get_project(project_id)
    base_image = project.snapshot_image or "your-registry/agent-sandbox:latest"

    container = docker.containers.run(
        image=base_image,
        name=f"sandbox-{project_id}",
        detach=True,
        volumes={f"vol-{project_id}": {"bind": "/home/agent", "mode": "rw"}},
        ports={"22/tcp": ssh_port},   # ttyd NOT exposed externally — only reachable via proxy
        environment={
            "PROJECT_ID":     str(project_id),
            "GCS_BUCKET":     GCS_BUCKET,
            "GCS_PREFIX":     f"projects/{project_id}",
            "GCS_SA_KEY":     sa_key,
            "SSH_PUBLIC_KEY": public_key,
            "HTTP_PROXY":     "http://host.docker.internal:3128",
            "HTTPS_PROXY":    "http://host.docker.internal:3128",
            "NO_PROXY":       "localhost,127.0.0.1",
        },
        extra_hosts={"host.docker.internal": "host-gateway"},
        network=f"net-{project_id}",
        cap_add=["SYS_ADMIN"],
        devices=["/dev/fuse"],
        security_opt=["apparmor:unconfined"],
        mem_limit="1g",
        nano_cpus=1_000_000_000,
    )

    # 7. Update DB
    update_project(project_id, container_id=container.id, status="running")

    return project_id, ssh_port, private_key
```

### 5.2 Snapshot (on inactivity or manual stop)

Two things happen on snapshot:
- **docker commit → Artifact Registry**: captures system state — installed packages, global tools, configs, shell history. Does NOT capture `/home/agent` contents (the volume mounts over the image layer on restore, so image content at `/home/agent` is irrelevant).
- **rclone sync → GCS**: captures `/home/agent` file contents as disaster recovery. Used only if the Docker volume is lost or corrupted — not part of the normal restore flow.

On restore: pull the image (system state) + attach the existing named volume (files). Both are needed. GCS is the fallback only.

```python
REGISTRY = "{region}-docker.pkg.dev/{gcp_project}/sandboxes"

async def snapshot_project(project_id):
    update_project(project_id, status="snapshotting")
    container = get_container(project_id)

    # 1. Final rclone sync → GCS (file-level backup, disaster recovery)
    container.exec_run(
        f"rclone sync /home/agent :gcs:{GCS_BUCKET}/projects/{project_id}/workspace "
        "--transfers=8 --checksum",
        user="agent"
    )
    update_project(project_id, last_backup_at=now())

    # 2. docker commit → image tagged with timestamp + :latest
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    image_tag = f"{REGISTRY}/{project_id}:{timestamp}"
    latest_tag = f"{REGISTRY}/{project_id}:latest"

    image = container.commit(repository=f"{REGISTRY}/{project_id}", tag=timestamp)
    image.tag(f"{REGISTRY}/{project_id}", tag="latest")

    # 3. Push both tags to Artifact Registry
    docker_client.images.push(f"{REGISTRY}/{project_id}", tag=timestamp)
    docker_client.images.push(f"{REGISTRY}/{project_id}", tag="latest")

    # 4. Update DB
    update_project(project_id,
                   snapshot_image=latest_tag,
                   last_snapshot_at=now())

    # 5. Stop and remove container (frees VM resources)
    container.stop(timeout=30)
    container.remove()
    update_project(project_id, status="stopped")
```

**Artifact Registry tag retention** — configure a cleanup policy on the `sandboxes` repository to keep only the last 5 tags per project. Old snapshots are deleted automatically.

### 5.3 Inactivity detection

A background task in the Project Service checks every 5 minutes:

```python
INACTIVITY_TIMEOUT_MINUTES = 30

async def inactivity_check():
    while True:
        running_projects = get_projects_by_status("running")
        for project in running_projects:
            idle_minutes = (now() - project.last_connection_at).seconds / 60
            if idle_minutes >= INACTIVITY_TIMEOUT_MINUTES:
                await snapshot_project(project.id)
        await asyncio.sleep(300)
```

`last_connection_at` is updated by the Project Service every time a ttyd WebSocket connection is established.

### 5.4 Restore

Fast path uses the Artifact Registry snapshot image — the container starts with full system state already baked in. The `/home/agent/.sandbox_initialized` flag lives in the Docker volume (not the image), so it persists across container restarts but gets reset if the volume is lost — which is intentional: losing the volume triggers a fresh GCS restore on next boot.

```python
async def restore_project(project_id, user_id):
    assert_owns(user_id, project_id)
    project = get_project(project_id)
    update_project(project_id, status="restoring")

    ssh_port = find_free_port()

    if project.snapshot_image:
        # Fast path — pull committed image, full state baked in
        docker_client.images.pull(project.snapshot_image)
        image = project.snapshot_image
    else:
        # Fallback — fresh base image, entrypoint restores from GCS
        image = "your-registry/agent-sandbox:latest"

    container = docker_client.containers.run(
        image=image,
        name=f"sandbox-{project_id}",
        detach=True,
        volumes={f"vol-{project_id}": {"bind": "/home/agent", "mode": "rw"}},
        ports={"22/tcp": ssh_port},   # ttyd NOT exposed externally — only reachable via proxy
        environment={...},   # same as create (includes HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
        extra_hosts={"host.docker.internal": "host-gateway"},
        network=f"net-{project_id}",
        cap_add=["SYS_ADMIN"],
        devices=["/dev/fuse"],
        security_opt=["apparmor:unconfined"],
        mem_limit="1g",
        nano_cpus=1_000_000_000,
    )

    update_project(project_id,
                   container_id=container.id,
                   ssh_host_port=ssh_port,
                   status="running")
```

### 5.5 Delete

```python
async def delete_project(project_id, user_id):
    assert_owns(user_id, project_id)

    if is_running(project_id):
        container = get_container(project_id)
        container.stop(timeout=30)
        container.remove()

    remove_volume(f"vol-{project_id}")
    delete_gcp_service_account(project_id)

    # Delete all Artifact Registry images for this project
    delete_artifact_registry_images(f"{REGISTRY}/{project_id}")

    # GCS files at gs://bucket/projects/{id}/ cleaned up by 30-day lifecycle rule

    delete_project_record(project_id)
```

---

## 6. Sandbox Base Image

### Dockerfile

```dockerfile
FROM ubuntu:24.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    openssh-server \
    python3 python3-pip python3-venv \
    nodejs npm \
    git curl wget jq \
    fuse3 \
    supervisor \
    tmux \
    && rm -rf /var/lib/apt/lists/*

# ttyd — exposes terminal over WebSocket
RUN wget -O /usr/local/bin/ttyd \
    https://github.com/tsl0922/ttyd/releases/latest/download/ttyd.x86_64 \
    && chmod +x /usr/local/bin/ttyd

# gcsfuse
RUN curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg \
    | gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg \
    && echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] \
    https://packages.cloud.google.com/apt gcsfuse-noble main" \
    > /etc/apt/sources.list.d/gcsfuse.list \
    && apt-get update && apt-get install -y gcsfuse

# rclone
RUN curl https://rclone.org/install.sh | bash

# Claude Code (requires Node.js)
RUN npm install -g @anthropic-ai/claude-code

# agent user
RUN useradd -m -s /bin/bash agent \
    && mkdir -p /home/agent/.ssh \
    && chmod 700 /home/agent/.ssh \
    && chown -R agent:agent /home/agent

RUN mkdir /var/run/sshd

COPY config/sshd_config          /etc/ssh/sshd_config
COPY config/supervisord.conf     /etc/supervisor/conf.d/supervisord.conf
COPY entrypoint.sh               /entrypoint.sh

RUN chmod +x /entrypoint.sh

HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
  CMD supervisorctl status | grep -v RUNNING && exit 1 || exit 0

EXPOSE 22 7681

ENTRYPOINT ["/entrypoint.sh"]
```

### entrypoint.sh

```bash
#!/bin/bash
set -e

# --- SSH key ---
echo "$SSH_PUBLIC_KEY" > /home/agent/.ssh/authorized_keys
chmod 600 /home/agent/.ssh/authorized_keys
chown agent:agent /home/agent/.ssh/authorized_keys

# --- GCS service account key ---
echo "$GCS_SA_KEY" > /tmp/gcs-key.json
chmod 600 /tmp/gcs-key.json
export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcs-key.json

# --- Mount GCS (project-scoped, read-write) ---
mkdir -p /mnt/gcs /mnt/shared
gcsfuse \
    --key-file=/tmp/gcs-key.json \
    --uid=$(id -u agent) --gid=$(id -g agent) \
    --implicit-dirs \
    --only-dir="projects/${PROJECT_ID}" \
    "${GCS_BUCKET}" /mnt/gcs

# --- Mount shared GCS (read-only) ---
gcsfuse \
    --key-file=/tmp/gcs-key.json \
    --uid=$(id -u agent) --gid=$(id -g agent) \
    --implicit-dirs \
    -o ro \
    --only-dir="shared" \
    "${GCS_BUCKET}" /mnt/shared

# --- First-boot restore from GCS ---
INIT_FLAG="/home/agent/.sandbox_initialized"
if [ ! -f "$INIT_FLAG" ]; then
    echo "First boot: checking for GCS backup..."
    if rclone ls ":gcs:${GCS_BUCKET}/projects/${PROJECT_ID}/workspace" \
       --contimeout=5s --timeout=10s 2>/dev/null | grep -q .; then
        echo "Backup found — restoring..."
        rclone sync \
            ":gcs:${GCS_BUCKET}/projects/${PROJECT_ID}/workspace" \
            /home/agent \
            --transfers=8 --checksum
    else
        echo "No backup found — fresh start."
    fi
    touch "$INIT_FLAG"
    chown agent:agent "$INIT_FLAG"
fi

exec /usr/bin/supervisord -n -c /etc/supervisor/conf.d/supervisord.conf
```

### supervisord.conf

```ini
[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log

[program:sshd]
command=/usr/sbin/sshd -D
autostart=true
autorestart=true

[program:ttyd]
command=ttyd --port 7681 --writable tmux new-session -A -s main
user=agent
autostart=true
autorestart=true

[program:backup-daemon]
command=python3 /opt/agent/backup_daemon.py
user=agent
autostart=true
autorestart=true
environment=HOME="/home/agent",GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcs-key.json",
            GCS_BUCKET="%(ENV_GCS_BUCKET)s",PROJECT_ID="%(ENV_PROJECT_ID)s"
```

### Backup daemon (replaces cron — inherits env vars cleanly)

```python
# /opt/agent/backup_daemon.py
import subprocess, time, os, logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")

BUCKET   = os.environ["GCS_BUCKET"]
PREFIX   = os.environ["PROJECT_ID"]
INTERVAL = int(os.environ.get("BACKUP_INTERVAL_SECONDS", "300"))

while True:
    try:
        result = subprocess.run([
            "rclone", "sync", "/home/agent",
            f":gcs:{BUCKET}/projects/{PREFIX}/workspace",
            "--transfers=4", "--checksum"
        ], capture_output=True, text=True)
        if result.returncode == 0:
            logging.info("Backup OK")
        else:
            logging.error(f"Backup failed: {result.stderr}")
    except Exception as e:
        logging.error(f"Backup exception: {e}")
    time.sleep(INTERVAL)
```

---

## 7. Networking

Docker containers get their own **kernel-level network namespace** — this is real OS-level isolation, not software-level. The building blocks below use this properly.

### 7.1 Per-container bridge networks (container-to-container isolation)

Each sandbox gets its own dedicated bridge network, not a shared one. Containers on different bridge networks cannot reach each other — Docker enforces this via iptables at the kernel level. IPv6 is explicitly disabled to prevent bypassing IPv4 iptables rules.

```python
# Project Service — on container create
ipam_config = docker.types.IPAMConfig(
    pool_configs=[docker.types.IPAMPool(subnet=f"172.{base}.0.0/24")]
)
network = docker_client.networks.create(
    f"net-{project_id}",
    driver="bridge",
    ipam=ipam_config,
    enable_ipv6=False,   # prevent IPv6 bypass of iptables rules
)
container = docker_client.containers.run(
    ...,
    network=f"net-{project_id}",
)

# Project Service — on container delete
docker_client.networks.get(f"net-{project_id}").remove()
```

Belt-and-suspenders: a global ip6tables rule on the host blocks all IPv6 forwarding as a fallback:
```bash
ip6tables -A FORWARD -j DROP
```

The `platform-net` (Project Service, Postgres) is on a completely separate network. Sandboxes cannot reach it at all. Squid runs on the host directly — not in Docker.

### 7.2 Explicit proxy + iptables lockdown

Squid runs as a **systemd service on the host VM** — not in Docker. Plain forward proxy — no ssl_bump, no CA certificates.

Each container gets `HTTP_PROXY` / `HTTPS_PROXY` env vars pointing at Squid. Tools (pip, npm, curl, git, Claude API SDK) see the env vars and send requests to Squid using the HTTP CONNECT method. Squid reads the domain from the CONNECT request, checks the per-project ACL, allows or blocks. If allowed, Squid creates a TCP tunnel — TLS passes through untouched, no MITM, no certificates.

**Why explicit proxy, not transparent interception (DNAT)?** DNAT rewrites the destination IP but the container sends a raw TLS ClientHello — Squid receives TLS bytes on its plain HTTP port and can't parse them. Fixing this requires `ssl_bump` with a CA certificate (MITM). That's complex and unnecessary. The explicit proxy approach uses the standard HTTP CONNECT method which works with all dev tools out of the box.

**iptables locks down the container to ONLY reach Squid** — nothing else on the host, nothing on the internet. Even if a process inside the container ignores the proxy env vars and tries a direct connection, the kernel drops it.

#### Custom iptables chains

Docker manages its own iptables rules in FORWARD (DOCKER, DOCKER-USER, DOCKER-ISOLATION chains). Inserting raw rules with `-I FORWARD 1` is fragile — Docker can clobber them on daemon restart or network recreation. Custom chains solve this — Docker doesn't touch chains it didn't create.

**One-time setup** (Terminal Proxy runs this on startup):
```bash
# Create custom chains (idempotent — errors ignored if they already exist)
iptables -N SANDBOX-INPUT  2>/dev/null || true
iptables -N SANDBOX-FORWARD 2>/dev/null || true

# Jump into our chains from the main chains (check before inserting to avoid duplicates)
iptables -C INPUT -j SANDBOX-INPUT 2>/dev/null || iptables -I INPUT 1 -j SANDBOX-INPUT
iptables -C FORWARD -j SANDBOX-FORWARD 2>/dev/null || iptables -I FORWARD 1 -j SANDBOX-FORWARD
```

#### Per-container rules (3 rules each)

```bash
CONTAINER_IP=$(docker inspect sandbox-{project_id} \
    --format '{{.NetworkSettings.Networks.net-{project_id}.IPAddress}}')
GATEWAY_IP=$(docker inspect sandbox-{project_id} \
    --format '{{.NetworkSettings.Networks.net-{project_id}.Gateway}}')

# Rule 1: Allow container → host gateway on TCP port 3128 ONLY (Squid)
iptables -A SANDBOX-INPUT -s $CONTAINER_IP -d $GATEWAY_IP -p tcp --dport 3128 -j ACCEPT

# Rule 2: Block container → host on ANY other port (Project Service, Postgres, SSH, etc.)
iptables -A SANDBOX-INPUT -s $CONTAINER_IP -j DROP

# Rule 3: Block container → internet directly (bypass attempts)
iptables -A SANDBOX-FORWARD -s $CONTAINER_IP -j DROP
```

**What each rule blocks:**

| Traffic | Chain | Rule | Result |
|---------|-------|------|--------|
| Container → Squid (gateway:3128) | INPUT | Rule 1 ACCEPT | Allowed |
| Container → Project Service (host:8000) | INPUT | Rule 2 DROP | Blocked |
| Container → PostgreSQL (host:5432) | INPUT | Rule 2 DROP | Blocked |
| Container → any host port except 3128 | INPUT | Rule 2 DROP | Blocked |
| Container → internet (any IP) | FORWARD | Rule 3 DROP | Blocked |
| Container → other container | FORWARD | Rule 3 DROP | Blocked |

**Why it can't be bypassed from inside:**
- `unset HTTP_PROXY` → tools try direct connections → FORWARD DROP
- Raw socket code → FORWARD DROP
- Discovers host public IP, tries port 8000 → INPUT DROP (not gateway:3128)
- Tries another bridge's gateway → routes through FORWARD → DROP
- ICMP (ping) → INPUT DROP (rule 1 only allows TCP/3128)
- DNS → Docker's embedded DNS (127.0.0.11) is inside the container namespace, never hits iptables

#### Cleanup on container delete

```bash
iptables -D SANDBOX-INPUT -s $CONTAINER_IP -d $GATEWAY_IP -p tcp --dport 3128 -j ACCEPT
iptables -D SANDBOX-INPUT -s $CONTAINER_IP -j DROP
iptables -D SANDBOX-FORWARD -s $CONTAINER_IP -j DROP
```

Deletes by exact match. Only affects this container's IP. Other containers' rules are untouched.

#### Squid configuration

**squid.conf** — one-time static setup:
```
http_port 3128
include /etc/squid/conf.d/*.conf
http_access deny all
```

The `include` loads all per-project fragments before the final `deny all`.

**Per-project fragment** written by Terminal Proxy to `/etc/squid/conf.d/project-{id}.conf`:
```
acl project_{id} src {container_ip}/32
acl project_{id}_domains dstdomain "/etc/squid/acls/project-{id}.acl"
http_access allow CONNECT project_{id} project_{id}_domains
http_access allow project_{id} project_{id}_domains
http_access deny project_{id}
```

- `allow CONNECT ... project_{id}_domains` — allows HTTPS tunneling to allowed domains
- `allow ... project_{id}_domains` — allows plain HTTP to allowed domains
- `deny project_{id}` — blocks everything else from this container

**Per-project domain allowlist** at `/etc/squid/acls/project-{id}.acl`:
```
api.anthropic.com
storage.googleapis.com
pypi.org
files.pythonhosted.org
github.com
registry.npmjs.org
```

Note: Squid's `dstdomain` matches the domain AND all subdomains. `github.com` in the allowlist also allows `*.github.com`.

Terminal Proxy writes both files on container create (using atomic write-then-rename to prevent Squid from reading partial files) and deletes both on container delete. Reload via `SIGHUP` — Squid re-reads all config files. If new config is valid, applies immediately (existing TCP tunnels continue uninterrupted). If syntax error, Squid keeps the old config — no crash, no downtime for any project.

**Why host Squid is better than Docker Squid here:**
- No inter-container lateral movement risk — Squid cannot reach into sandbox networks
- Reachable from all container networks via the bridge gateway IP — no dynamic network attachment needed
- One process on the host, not a container that itself could be escaped

#### Serialization

Sandbox create/delete operations are serialized with a lock in Terminal Proxy. At 100 users, contention is negligible. This prevents interleaved iptables commands or Squid config writes from concurrent operations.

#### DNS tunneling — accepted risk for v1

Docker's embedded DNS (127.0.0.11) forwards queries to the host's upstream resolver. This traffic originates from the Docker daemon (host process), not the container — iptables doesn't see it. A container could theoretically encode data in DNS queries. DNS tunneling is extremely low bandwidth (~50 bytes/query), requires the attacker to control a nameserver, and is not a practical concern for v1 at 100 users. Phase 2 mitigation: local DNS resolver (unbound) that only resolves domains in each project's allowlist.

#### GCP firewall — perimeter defense

Port 3128 must **never** be opened in GCP firewall rules. Even though Squid binds on `0.0.0.0:3128`, no traffic from the public internet can reach it. Only traffic from Docker bridge networks (inside the VM) reaches Squid. This prevents external attackers from using Squid as an open proxy.

GCP firewall rules should only open:
- Port 22 (SSH for admin access)
- Port 8000 (Project Service API for web/mobile clients)
- Mapped SSH ports for sandbox access (dynamic range, e.g., 10000–11000)

### 7.3 Bandwidth limiting (tc)

Linux traffic control (`tc`) on the container's veth interface limits throughput per sandbox:

```bash
# Find the veth interface for this container on the host
CONTAINER_PID=$(docker inspect {project_id} --format '{{.State.Pid}}')
VETH=$(nsenter -t $CONTAINER_PID -n ip link show eth0 | \
       awk '/eth0/{print $2}' | \
       xargs -I{} ip link show | grep -B1 {} | grep veth | awk '{print $2}')

# Limit to 10mbit outbound
tc qdisc add dev $VETH root tbf rate 10mbit burst 32kbit latency 400ms
```

Terminal Proxy runs these alongside container lifecycle — called by Project Service via internal HTTP on create/delete, same pattern as iptables rules.

### 7.4 WebSocket proxy (terminal interception)

The Terminal Proxy is a **dedicated Docker container running with `network_mode: host`**. It is completely separate from the Project Service and runs as its own standalone asyncio WebSocket server on port 9000.

Being on the host network means it:
- Shares the VM's full network stack — sees all Docker bridge interfaces
- Can reach any container's bridge IP (`net-{project_id}`) directly — no port mapping needed
- Can write `/etc/squid/acls/` and `/etc/squid/conf.d/`, send `SIGHUP` to Squid via shared PID namespace
- Runs in trusted territory — completely outside all sandbox containers, users cannot tamper with it

**Full connection and auth flow:**

```
CLIENT APP (mobile / web)
    │
    │  ws://{vm_public_ip}:9000/terminal/{project_id}?token={jwt}
    │  Port 9000 is owned by the Terminal Proxy via network_mode: host
    ▼
VM — TERMINAL PROXY CONTAINER (network_mode: host)
    │  Process: Python asyncio WebSocket server
    │  Shares VM's network stack — sees all bridge interfaces
    │
    │  Step 1: validate token — HTTP to localhost:8000
    │  (Project Service maps port 8000 to VM localhost,
    │   reachable from host-networked proxy)
    ▼
VM — DOCKER PLATFORM-NET
    ▼
PROJECT SERVICE CONTAINER (platform-net)
    │  Process: FastAPI /internal/validate endpoint
    │  - decodes JWT using JWT_SECRET
    │  - checks DB: does this user own this project?
    │  - returns 200 + user_id, or 401
    │
    back to Terminal Proxy via localhost:8000
    │
    ├── If 401 → close WebSocket. Client never reaches sandbox.
    │
    └── If 200 → look up sandbox container bridge IP via Docker SDK
                 e.g. 172.20.0.5 on net-{project_id}
                 Open ws://172.20.0.5:7681/ws (VM-internal, never leaves VM)
                 │
                 ▼
VM — DOCKER BRIDGE net-{project_id}
                 ▼
SANDBOX CONTAINER
    │  Process: ttyd on port 7681
    │  ttyd → tmux → bash / claude
    │  No knowledge of JWT or auth — just receives forwarded bytes
    │
    After auth: proxy is a transparent byte forwarder
    user types → bytes → proxy (logs) → ttyd → tmux pty → bash
    output bytes ← proxy (forwards as-is) ← ttyd ← bash stdout
```

ttyd binds on `0.0.0.0:7681` inside the container but **the port is not mapped in `docker run`** — unreachable from anywhere except the host-networked proxy via the container's bridge IP.

**Project Service — localhost-only middleware for `/internal/*`:**

Port 8000 is public (mobile/web clients need it). `/internal/*` routes are protected by middleware that rejects requests from any source other than `127.0.0.1`:

```python
# project-service/middleware.py
from fastapi import Request
from fastapi.responses import JSONResponse

@app.middleware("http")
async def restrict_internal_routes(request: Request, call_next):
    if request.url.path.startswith("/internal/"):
        client_ip = request.client.host
        if client_ip != "127.0.0.1":
            return JSONResponse(status_code=404)  # 404 not 403 — don't reveal the route exists
    return await call_next(request)
```

**Project Service — internal validate endpoint:**

```python
# project-service/routes/internal.py
@app.post("/internal/validate")
async def validate_terminal_token(body: ValidateRequest):
    user_id = decode_jwt(body.token)       # raises if invalid or expired
    project = get_project(body.project_id)
    if project.user_id != user_id:
        raise HTTPException(status_code=401)
    update_last_connection(body.project_id)
    return {"user_id": str(user_id)}
```

**Terminal Proxy — proxy.py:**

```python
import asyncio, httpx, websockets, docker
from websockets.asyncio.server import serve

PROJECT_SERVICE_URL = "http://localhost:8000"
LISTEN_PORT         = 9000
docker_client       = docker.from_env()

async def handle(client_ws, path):
    project_id = extract_project_id(path)
    token      = extract_token(path)

    # Step 1: validate JWT + ownership via Project Service
    async with httpx.AsyncClient() as http:
        resp = await http.post(
            f"{PROJECT_SERVICE_URL}/internal/validate",
            json={"token": token, "project_id": project_id}
        )
    if resp.status_code != 200:
        await client_ws.close(code=4401, reason="Unauthorized")
        return

    user_id = resp.json()["user_id"]

    # Step 2: look up container bridge IP
    container    = docker_client.containers.get(f"sandbox-{project_id}")
    container_ip = container.attrs["NetworkSettings"]["Networks"][
                       f"net-{project_id}"]["IPAddress"]

    # Step 3: connect to ttyd inside sandbox
    async with websockets.connect(f"ws://{container_ip}:7681/ws") as ttyd_ws:

        async def client_to_ttyd():
            async for message in client_ws:
                log_input(project_id, user_id, message)   # tamper-proof audit
                await ttyd_ws.send(message)

        async def ttyd_to_client():
            async for message in ttyd_ws:
                await client_ws.send(message)

        await asyncio.gather(client_to_ttyd(), ttyd_to_client())

async def main():
    async with serve(handle, "0.0.0.0", LISTEN_PORT):
        await asyncio.get_event_loop().run_forever()

asyncio.run(main())
```

**Squid ACL management** also lives in the Terminal Proxy since it needs host access:

```python
import os, signal, tempfile, pathlib

def _atomic_write(path: str, content: str):
    """Write to temp file then rename — prevents Squid from reading partial files."""
    dir_path = os.path.dirname(path)
    fd, tmp_path = tempfile.mkstemp(dir=dir_path)
    try:
        with os.fdopen(fd, "w") as f:
            f.write(content)
        os.rename(tmp_path, path)  # atomic on same filesystem
    except:
        os.unlink(tmp_path)
        raise

def _reload_squid():
    pid = int(open("/var/run/squid.pid").read().strip())
    os.kill(pid, signal.SIGHUP)

def update_squid_acl(project_id: str, allowed_domains: list[str]):
    _atomic_write(
        f"/etc/squid/acls/project-{project_id}.acl",
        "\n".join(allowed_domains) + "\n"
    )
    _reload_squid()

def remove_squid_acl(project_id: str):
    pathlib.Path(f"/etc/squid/acls/project-{project_id}.acl").unlink(missing_ok=True)
    pathlib.Path(f"/etc/squid/conf.d/project-{project_id}.conf").unlink(missing_ok=True)
    _reload_squid()
```

**What you can do at the intercept points:**

- **Tamper-proof audit log** — input written from a trusted process outside the sandbox
- **Rate limiting** — detect and throttle runaway command loops
- **Command blocking** — buffer until `\r`, parse full command, reject patterns
- **Session recording** — write full terminal I/O to GCS as asciinema (replayable)
- **Alerting** — flag suspicious commands in real time

### 7.5 Port routing

The Terminal Proxy owns port 9000 on the VM. The Project Service owns port 8000 — publicly accessible for mobile/web clients to reach the REST API. `/internal/*` routes are protected by middleware that only allows requests from `127.0.0.1` (see section 7.4). SSH is directly mapped per container:

```
terminal:  ws://{vm_public_ip}:9000/terminal/{project_id}?token={jwt}
           → Terminal Proxy (host network)
           → validates via localhost:8000 (Project Service)
           → ws://{container_bridge_ip}:7681 (ttyd, internal only)

ssh:       {vm_public_ip}:{ssh_host_port}
           → container:22 (direct Docker port mapping)
```

### 7.6 SSH access

```
ssh agent@{vm_external_ip} -p {ssh_host_port} -i {private_key_file}
```

Private key stored in device Keychain (iOS) / Android Keystore by the mobile app.

### 7.7 What Docker cannot protect against

Network isolation with Docker is solid. The real security gap is the **shared kernel** — all containers on the host share the same Linux kernel. A kernel exploit in one container can potentially break out to the host and affect all other sandboxes. This is not a networking problem.

Mitigations for phase 1:
- Seccomp profile — restricts which syscalls containers can make
- AppArmor profile — MAC policy limiting file and network access
- Drop all Linux capabilities except what's needed (`--cap-drop=ALL --cap-add=SYS_ADMIN` for gcsfuse only)

The complete fix is Phase 2 (Firecracker microVMs) — each sandbox gets its own kernel, so a kernel exploit in one sandbox is fully contained.

---

## 8. Docker Compose (Platform Infrastructure)

```yaml
version: "3.9"

services:

  project-service:
    build: ./project-service
    ports:
      - "8000:8000"   # public — mobile/web clients reach REST API here
                      # /internal/* routes protected by middleware (see below)
    environment:
      DATABASE_URL:   "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/sandboxes"
      JWT_SECRET:     "${JWT_SECRET}"
      GCS_BUCKET:     "${GCS_BUCKET}"
      GCP_PROJECT_ID: "${GCP_PROJECT_ID}"
      GOOGLE_APPLICATION_CREDENTIALS: "/secrets/project-service-sa.json"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./secrets/project-service-sa.json:/secrets/project-service-sa.json:ro
    networks: [platform-net]
    depends_on: [postgres]

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB:       sandboxes
      POSTGRES_USER:     "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
    volumes: [postgres-data:/var/lib/postgresql/data]
    networks: [platform-net]

  terminal-proxy:
    build: ./terminal-proxy
    network_mode: host          # shares VM network stack — reaches all container bridge IPs + iptables
    pid: "host"                 # shares host PID namespace — required for os.kill(squid_pid, SIGHUP)
    cap_add: [NET_ADMIN]        # required for iptables and tc commands
    environment:
      PROJECT_SERVICE_URL: "http://localhost:8000"
      LISTEN_PORT:         "9000"
      # No DATABASE_URL — proxy never queries DB directly, validates via Project Service HTTP
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro   # to look up container bridge IPs
      - /etc/squid/acls:/etc/squid/acls                # to write per-project ACL files
      - /etc/squid/conf.d:/etc/squid/conf.d            # to write per-project Squid conf fragments
      - /var/run/squid.pid:/var/run/squid.pid:ro        # to read Squid PID for SIGHUP
    depends_on: [project-service]

  # Squid is NOT in Docker Compose — runs as a systemd service on the host VM
  # Install: apt-get install squid
  # Config:  /etc/squid/squid.conf (includes /etc/squid/conf.d/*.conf)
  # ACLs:    /etc/squid/acls/project-{id}.acl  (written by Terminal Proxy)
  # Reload:  SIGHUP (sent by Terminal Proxy via os.kill)

volumes:
  postgres-data:

networks:
  platform-net:
    name: platform-net
    # No shared sandbox-net — each sandbox gets its own bridge network (net-{project_id})
    # created/destroyed dynamically by the Project Service
```

**Three services in Docker Compose:**
- `project-service` — REST API, lifecycle management, JWT issuance. On `platform-net`, not reachable from internet directly.
- `postgres` — database. On `platform-net` only.
- `terminal-proxy` — WebSocket proxy, auth enforcement, audit logging, Squid ACL management. On host network.

Sandbox containers are spawned dynamically by Project Service via Docker SDK, each on its own `net-{project_id}` bridge network.

---

## 9. GCP Setup Checklist

```bash
# 1. Create GCS bucket
gsutil mb -l europe-west1 gs://your-agent-bucket

# 2. Set 30-day lifecycle rule (auto-cleans deleted project data)
cat > lifecycle.json << 'EOF'
{ "rule": [{ "action": {"type": "Delete"},
             "condition": {"age": 30} }] }
EOF
gsutil lifecycle set lifecycle.json gs://your-agent-bucket

# 3. Create the Project Service service account
gcloud iam service-accounts create project-service-sa \
    --display-name="Agent Platform - Project Service"

# 4. Grant it permission to create/delete other service accounts
gcloud projects add-iam-policy-binding YOUR_GCP_PROJECT \
    --member="serviceAccount:project-service-sa@YOUR_GCP_PROJECT.iam.gserviceaccount.com" \
    --role="roles/iam.serviceAccountAdmin"

# 5. Grant it storage admin on the bucket (to set per-project IAM conditions)
gsutil iam ch \
    serviceAccount:project-service-sa@YOUR_GCP_PROJECT.iam.gserviceaccount.com:roles/storage.admin \
    gs://your-agent-bucket

# 6. Download Project Service SA key
gcloud iam service-accounts keys create ./secrets/project-service-sa.json \
    --iam-account=project-service-sa@YOUR_GCP_PROJECT.iam.gserviceaccount.com

# 7. Create Artifact Registry repository for sandbox images
gcloud artifacts repositories create sandboxes \
    --repository-format=docker \
    --location={region} \
    --description="Sandbox container snapshots"

# 8. Grant Project Service SA permission to push/pull images
gcloud artifacts repositories add-iam-policy-binding sandboxes \
    --location={region} \
    --member="serviceAccount:project-service-sa@..." \
    --role="roles/artifactregistry.writer"

# 9. Configure cleanup policy — keep last 5 snapshots per project
# (set via GCP Console: Artifact Registry → sandboxes → Edit → Cleanup policies)
# Policy: Keep most recent 5 versions, delete the rest

# 10. Enable required APIs
gcloud services enable iam.googleapis.com storage.googleapis.com artifactregistry.googleapis.com

# 11. Firewall rules — only open what's needed
#     Port 3128 (Squid) must NEVER be opened externally
gcloud compute firewall-rules create allow-platform \
    --allow=tcp:22,tcp:8000,tcp:9000,tcp:10000-11000 \
    --target-tags=sandbox-vm \
    --description="SSH + Project Service + Terminal Proxy + sandbox SSH ports"
```

---

## 10. Build Order

### Week 1 — Sandbox + Storage foundation
- [ ] Dockerfile + entrypoint.sh + supervisord.conf
- [ ] `docker run` → SSH into container, verify `/home/agent` is writable
- [ ] GCS bucket + lifecycle rule + Project Service SA setup (GCP checklist above)
- [ ] gcsfuse mount working inside container
- [ ] Backup daemon running — create a file, wait 5 min, verify it's in GCS
- [ ] Restore flow: delete volume → restore container → files come back

### Week 2 — Project Service
- [ ] FastAPI skeleton + Postgres schema
- [ ] `POST /projects` → spawn container, create GCP SA, return SSH creds + private key
- [ ] `POST /projects/{id}/stop` → final backup → stop
- [ ] `POST /projects/{id}/start` → start (first-boot restore logic)
- [ ] `DELETE /projects/{id}` → teardown + SA deletion + Artifact Registry cleanup
- [ ] Snapshot on stop: `docker commit` → push to Artifact Registry + rclone sync
- [ ] Fast restore: pull snapshot image → run container
- [ ] Inactivity check background task (30 min idle → auto snapshot)
- [ ] JWT auth middleware + `/auth/register` + `/auth/login`

### Week 3 — Networking + Terminal Proxy + Claude Code
- [ ] Terminal Proxy service — asyncio WebSocket server, JWT validation via Project Service
- [ ] Per-container bridge networks (IPv6 disabled) + custom iptables chains (SANDBOX-INPUT, SANDBOX-FORWARD)
- [ ] Squid installed on host as systemd service, base squid.conf with include + deny all
- [ ] Per-container iptables rules (3 per container: INPUT ACCEPT gateway:3128, INPUT DROP, FORWARD DROP)
- [ ] Squid per-project conf fragments + ACL files with atomic writes, reload via SIGHUP
- [ ] ip6tables FORWARD DROP as belt-and-suspenders IPv6 block
- [ ] GCP firewall: verify port 3128 is NOT open externally
- [ ] Claude Code installed in image (`npm install -g @anthropic-ai/claude-code`)
- [ ] ttyd running in image, verify terminal loads via proxy in browser
- [ ] `ANTHROPIC_API_KEY` injected into container, `claude` command works
- [ ] `GET /projects/{id}` returns `terminal_url` (proxy URL, port 9000)

### Week 4 — Web + Mobile clients
- [ ] Auth screens (register / login) → JWT stored in secure storage
- [ ] Projects list screen
- [ ] New Project flow → creates project → private key stored in Keychain
- [ ] Project detail screen (terminal URL, SSH string, status, last backup time)
- [ ] Terminal view connecting to `terminal_url`

---

## 11. Terminal Architecture

### 11.1 Tool overview

**ttyd**
A small, self-contained binary (~2MB) that exposes a real PTY (pseudo-terminal) over WebSocket. It spawns a shell process inside the container and proxies all input/output over the WebSocket connection. It ships with its own built-in xterm.js frontend, so pointing a WebView or browser tab at `http://host:7681` gives a fully working terminal with zero extra frontend code. It supports resize events (SIGWINCH), colour, UTF-8, and all terminal escape codes.

**xterm.js**
The de-facto browser terminal emulator. Renders a VT100/xterm-compatible terminal in a `<canvas>` element. Used by VS Code, GitHub Codespaces, Hyper, and ttyd's built-in frontend. When you want custom styling or tighter integration with your own UI, you replace ttyd's built-in page with your own xterm.js component — you still connect it to ttyd's WebSocket, xterm.js just handles the rendering and keyboard input instead of ttyd's bundled page.

**libtmux / pexpect / ptyprocess**
Not used in this architecture. Claude Code manages its own execution context natively. These libraries would only be relevant if you were building a custom agent process that needed to drive interactive programs programmatically.

---

### 11.2 Sandbox

ttyd and tmux are already in the base image (section 6) — no separate additions needed. ttyd runs under supervisor pointing at a named tmux session (`main`). Claude Code runs inside that session — all activity is visible in the terminal view in real time.

---

### 11.3 Project Service changes

ttyd is not mapped to any host port. The proxy looks up the container's Docker network IP at connection time. The API response returns the proxy URL:

```json
{
  "project_id": "abc123",
  "status": "running",
  "terminal_url": "ws://34.x.x.x:9000/terminal/abc123?token=...",
  "ssh": { "host": "34.x.x.x", "port": 32456, "user": "agent" }
}
```

---

### 11.4 Web client (separate repo: `sandbox-web`)

**Stack**: React + xterm.js + xterm-addon-fit + xterm-addon-web-links

Using your own xterm.js component gives you control over theme, font, toolbar, and layout. The WebSocket connects to the Terminal Proxy (port 9000), which forwards to ttyd inside the sandbox after JWT validation.

```
sandbox-web/
  src/
    components/
      Terminal.tsx        ← xterm.js component, connects to ttyd WebSocket
      ProjectList.tsx
      ProjectDetail.tsx
    api/
      client.ts           ← typed fetch wrapper for Project Service API
    App.tsx
  package.json
```

**Terminal.tsx**:

```tsx
import { useEffect, useRef } from "react"
import { Terminal } from "@xterm/xterm"
import { FitAddon } from "@xterm/addon-fit"
import { WebLinksAddon } from "@xterm/addon-web-links"
import { AttachAddon } from "@xterm/addon-attach"
import "@xterm/xterm/css/xterm.css"

interface Props {
  terminalUrl: string   // e.g. "ws://34.x.x.x:9000/terminal/{project_id}?token={jwt}"
}

export function TerminalView({ terminalUrl }: Props) {
  const containerRef = useRef<HTMLDivElement>(null)

  useEffect(() => {
    const term    = new Terminal({ theme: { background: "#1e1e1e" }, fontSize: 14 })
    const fit     = new FitAddon()
    const links   = new WebLinksAddon()

    term.loadAddon(fit)
    term.loadAddon(links)
    term.open(containerRef.current!)
    fit.fit()

    // terminal_url is already a ws:// URL pointing at the Terminal Proxy
    const socket = new WebSocket(terminalUrl)
    const attach = new AttachAddon(socket)
    term.loadAddon(attach)

    // Resize terminal when window resizes
    const observer = new ResizeObserver(() => fit.fit())
    observer.observe(containerRef.current!)

    // Tell Terminal Proxy / ttyd about resize
    socket.onopen = () => {
      socket.send(JSON.stringify({ type: "resize",
                                   cols: term.cols,
                                   rows: term.rows }))
    }

    return () => {
      term.dispose()
      socket.close()
      observer.disconnect()
    }
  }, [terminalUrl])

  return <div ref={containerRef} style={{ width: "100%", height: "100%" }} />
}
```

**Dependencies**:
```json
{
  "@xterm/xterm": "^5.5.0",
  "@xterm/addon-fit": "^0.10.0",
  "@xterm/addon-attach": "^0.11.0",
  "@xterm/addon-web-links": "^0.11.0"
}
```

**Build order for sandbox-web**:
- [ ] Auth screens (login / register) → store JWT in localStorage
- [ ] Projects list → `GET /projects`
- [ ] New Project modal → `POST /projects`
- [ ] Project detail page with `TerminalView` component
- [ ] Split-pane layout: terminal left, agent chat right

---

### 11.5 Mobile client (separate repo: `sandbox-mobile`)

**Stack**: React Native + WebView (renders a self-contained HTML page that connects to the Terminal Proxy)

`terminal_url` is a `ws://` WebSocket URL — it cannot be loaded directly in a WebView. Instead the mobile client loads a small self-contained HTML page (inlined or served from the Project Service) that boots xterm.js and opens a WebSocket to `terminal_url`. The WebView just hosts that page.

```
sandbox-mobile/
  src/
    screens/
      AuthScreen.tsx
      ProjectListScreen.tsx
      ProjectDetailScreen.tsx
      TerminalScreen.tsx      ← WebView pointing at terminal_url
    api/
      client.ts
    storage/
      keychain.ts             ← SSH private key + JWT in Keychain/Keystore
  App.tsx
  package.json
```

**TerminalScreen.tsx**:

```tsx
import { WebView } from "react-native-webview"
import { useRoute } from "@react-navigation/native"

export function TerminalScreen() {
  const { terminalUrl } = useRoute().params as { terminalUrl: string }

  // terminal_url is a ws:// URL — inject it into a self-contained HTML page
  // that boots xterm.js and connects the WebSocket.
  // The HTML page can be inlined here or fetched from the Project Service.
  const html = `
    <!DOCTYPE html>
    <html>
    <head>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/xterm/css/xterm.css"/>
      <style>body { margin:0; background:#1e1e1e; } #terminal { height:100vh; }</style>
    </head>
    <body>
      <div id="terminal"></div>
      <script src="https://cdn.jsdelivr.net/npm/xterm/lib/xterm.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/xterm-addon-fit/lib/xterm-addon-fit.js"></script>
      <script>
        const term = new Terminal({ theme: { background: '#1e1e1e' }, fontSize: 13 });
        const fit  = new FitAddon.FitAddon();
        term.loadAddon(fit);
        term.open(document.getElementById('terminal'));
        fit.fit();
        const ws = new WebSocket('${terminalUrl}');
        ws.onopen    = () => ws.send(JSON.stringify({ type:'resize', cols:term.cols, rows:term.rows }));
        ws.onmessage = e => term.write(typeof e.data === 'string' ? e.data : new Uint8Array(e.data));
        term.onData(data => ws.send(data));
        window.addEventListener('resize', () => fit.fit());
      </script>
    </body>
    </html>
  `

  return (
    <WebView
      source={{ html }}
      style={{ flex: 1 }}
      keyboardDisplayRequiresUserAction={false}
      mixedContentMode="always"
    />
  )
}
```

**If you want a native terminal feel** (keyboard toolbar, custom gestures), replace the inline HTML approach with a native xterm port (`react-native-xterm`) or a more polished embedded HTML page hosted on your own CDN.

**Dependencies**:
```json
{
  "react-native-webview": "^13.0.0",
  "@react-navigation/native": "^6.0.0",
  "react-native-keychain": "^8.0.0"
}
```

**Build order for sandbox-mobile**:
- [ ] Auth screens → JWT stored via react-native-keychain
- [ ] Projects list screen → `GET /projects`
- [ ] New Project screen → `POST /projects` → store private key in Keychain
- [ ] Project detail screen (terminal URL, SSH string, status, last backup)
- [ ] `TerminalScreen` — WebView pointing at `terminal_url`

---

### 11.6 Repository structure

```
sandbox-platform/
  backend/               ← Project Service (FastAPI) + sandbox image
  sandbox-web/           ← React web client
  sandbox-mobile/        ← React Native mobile client
  docker-compose.yml     ← platform infra (Project Service, Postgres, Terminal Proxy)
```

The three can share a monorepo root or be independent repos — the only coupling is the Project Service REST API contract, which both clients consume identically.

```
backend/
  project-service/    ← FastAPI app
  terminal-proxy/     ← asyncio WebSocket proxy
  sandbox/            ← Dockerfile + entrypoint + supervisord + backup_daemon.py
```

---

## 12. Phase 2 Migration (Docker → Firecracker microVMs)

Everything inside the sandbox is unchanged. Only the spawn mechanism in Project Service changes.

| Component | Phase 1 | Phase 2 |
|-----------|---------|---------|
| Sandbox spawn | `docker.containers.run(...)` | Firecracker API / VM pool |
| Isolation | Container (shared kernel) | microVM (dedicated kernel) |
| Resource limits | Partial (`--memory`, `--cpus`) | Fully enforced |
| Cold start | ~1–2s | ~200ms with snapshots |
| Everything inside sandbox | Identical | Identical |
| Project Service API | Identical | Identical |
| Mobile app | Identical | Identical |

The Project Service abstracts the compute layer. In phase 2 you replace one `spawn_container()` function with `spawn_microvm()`. No other code changes.

---

## 13. Security Notes (Phase 1)

| Risk | Mitigation |
|------|-----------|
| Container escape | Acceptable for launch; eliminated in phase 2 with microVMs |
| Cross-user data access | `user_id` filter on every DB query + per-project GCP SA with IAM conditions |
| Container reaching Project Service | Per-container bridge networks — sandboxes cannot reach each other or platform-net |
| Resource exhaustion | `--memory=1g --nano-cpus=1000000000` on container spawn |
| GCS SA key | Stored in plaintext in Postgres — known limitation, migrate to Secret Manager later; at runtime lives only in `/tmp/gcs-key.json` inside the container |
| JWT secret exposure | Never commit `JWT_SECRET` — use GCP Secret Manager or `.env` excluded from git |
| SSH private key | Stored in plaintext in Postgres for now — known limitation, migrate to Secret Manager later; returned to client on creation; client stores in device Keychain / localStorage for SSH access |